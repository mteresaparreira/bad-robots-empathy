{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e42600d",
   "metadata": {},
   "source": [
    "\n",
    "Description: This script trains a LSTM Model on the facial data extracted from the OpenFace library to classify human reactions into Control, Failure Human, Failure Robot classes  \n",
    "\n",
    "- Author: Sukruth Gowdru Lingaraju\n",
    "- Date Created: August 18th, 2023\n",
    "- Python Version: 3.10.9\n",
    "- Email: sg2257@cornell.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd487f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report,a confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TensorFlow is using GPU support\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493244d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Begin: Method Definitions\n",
    "\"\"\"\n",
    "# def classification_report_tolerance(y_pred, y_true, margin = 1):\n",
    "    \n",
    "#     metrics_dict = dict()\n",
    "#     all_metrics_dict = dict()\n",
    "#     classes = np.unique(y_true)\n",
    "#     all_p = []\n",
    "#     all_r = []\n",
    "#     all_a = []\n",
    "#     all_f1 = []\n",
    "    \n",
    "#     for classi in classes:\n",
    "#         tp = 0\n",
    "#         tn = 0\n",
    "#         fp = 0\n",
    "#         fn = 0\n",
    "\n",
    "\n",
    "#         for i, y in enumerate(y_pred):\n",
    "#             if y == classi:\n",
    "#                 if y in y_true[i-margin:i+margin+1]:\n",
    "#                     #print(y_true[i-1:i+1].shape)\n",
    "#                     tp = tp + 1\n",
    "#                 else:\n",
    "#                     fp = fp + 1\n",
    "#             else:\n",
    "#                 if y not in y_true[i-1:i+1]:\n",
    "#                     fn = fn + 1\n",
    "#                 else:\n",
    "#                     tn = tn + 1\n",
    "#         precision = tp/(tp+fp)\n",
    "#         recall = tp / (tp + fn)\n",
    "#         f1 = (2*precision*recall)/(precision+recall)\n",
    "#         accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "#         metrics_dict[classi] = [tp, tn, fp, fp]\n",
    "#         all_metrics_dict[classi] = [precision, recall, f1, accuracy]\n",
    "#         all_p.append(precision)\n",
    "#         all_r.append(recall)\n",
    "#         all_a.append(accuracy)\n",
    "#         all_f1.append(f1)\n",
    "        \n",
    "    \n",
    "#     print(metrics_dict)\n",
    "#     print(all_metrics_dict)\n",
    "    \n",
    "#     macro_dict = dict()\n",
    "#     macro_dict['macro-precision'] = sum(np.array(all_p))/len(all_p)\n",
    "#     macro_dict['macro-recall'] = sum(np.array(all_r))/len(all_r)\n",
    "#     macro_dict['macro-accuracy'] = sum(np.array(all_a))/len(all_a)\n",
    "#     macro_dict['macro-f1'] = (2* macro_dict['macro-precision']*macro_dict['macro-recall'])/(macro_dict['macro-recall'] + macro_dict['macro-precision'])\n",
    "    \n",
    "#     print(macro_dict)\n",
    "    \n",
    "#     return metrics_dict, all_metrics_dict, macro_dict\n",
    "\n",
    "def createDataSplits(df, results_directory= '.', seed_value = 42, sequence_length = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    createDataSplits(): accepts a dataframe along with the directory to store the results and sequence_length - to perform data splits for training, validation, and testing\n",
    "\n",
    "    Parameters:\n",
    "    - df\n",
    "    - results_directory: to write the 'Exception Error' thrown if there are any problems in splitting the data\n",
    "    - seed_value\n",
    "    - sequence_length (a.k.a: lookbacks)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # # Set seed\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        tf.random.set_seed(seed_value)\n",
    "\n",
    "        # # Extract features and labels\n",
    "\n",
    "        # # for naive & naive_n datasets\n",
    "        # features = df.iloc[:, 3:]\n",
    "        # target_class = df['class'].values\n",
    "\n",
    "        # # for full & full_n datasets\n",
    "        features = df.iloc[:, 4:]\n",
    "        target_class = df.iloc[:, 2].values\n",
    "        target_class = target_class.astype('int')\n",
    "        \n",
    "        \"\"\"\n",
    "        Begin: K-Fold Cross-Validation splits\n",
    "        \n",
    "        Identify the range of the splits & assign participants belonging to those ranges to their respective folds\n",
    "        - 'start_test_indx', 'end_test_indx': defines the range of the particiapants belonging to the 'test_fold'\n",
    "        - 'test_fold': consists of the participants belonging to the 'k'th fold\n",
    "        - 'remaining_participants': set difference between original 'participants' & 'test_fold' participants\n",
    "        - 'val_fold': consists of 'val_fold_size' participants randomly shuffled after obtaining 'remaining_participants' belonging to the 'k'th fold\n",
    "        - 'train_fold': consists of all the remaining participants belonging to the 'k'th fold\n",
    "        - 'test_folds', 'val_folds', 'train_folds': consists of the set of participants in each fold\n",
    "        \"\"\"\n",
    "        \n",
    "        participants = np.unique(df['participant_id'])\n",
    "\n",
    "        #Number of participants for train, validation, and test\n",
    "        train_fold_size = 20\n",
    "        val_fold_size = 3\n",
    "        test_fold_size = 6\n",
    "\n",
    "        #number of dataset folds\n",
    "        num_folds = 5\n",
    "\n",
    "        # Shuffle the list of participants\n",
    "        np.random.shuffle(participants)\n",
    "\n",
    "        # Initialize lists to store train, validation, and test participants for each fold\n",
    "        train_folds = []\n",
    "        val_folds = []\n",
    "        test_folds = []\n",
    "\n",
    "        # Create non-overlapping test folds and validation folds\n",
    "        for i in range(num_folds):\n",
    "            start_test_idx = i * test_fold_size\n",
    "            end_test_idx = start_test_idx + np.min([test_fold_size, len(participants) - start_test_idx])\n",
    "\n",
    "            test_fold = participants[start_test_idx:end_test_idx]\n",
    "               \n",
    "            # Identify all the participants except the participants belonging to the test_fold & shuffle them\n",
    "            remaining_participants = np.setdiff1d(participants, test_fold)\n",
    "            np.random.shuffle(remaining_participants)\n",
    "            \n",
    "            # Validation set selected from the remaining participants\n",
    "            val_fold = remaining_participants[:val_fold_size]\n",
    "            \n",
    "            # Identify all the participants that don't belong to 'val_fold' & 'test_fold' and assign them to the 'train_fold'\n",
    "            train_fold = np.setdiff1d(remaining_participants, val_fold)\n",
    "            \n",
    "            # Append the participant sets to their corresponding folds\n",
    "            train_folds.append(train_fold)\n",
    "            val_folds.append(val_fold)\n",
    "            test_folds.append(test_fold)\n",
    "        \n",
    "        \"\"\"\n",
    "        End: K-Fold Cross-Validation splits\n",
    "        \"\"\"\n",
    "\n",
    "        # # Create train, validation, and test sets for each fold in 'num_folds'\n",
    "        # For now, do only for fold: '0'\n",
    "        train_fold = train_folds[0]\n",
    "        val_fold = val_folds[0]\n",
    "        test_fold = test_folds[0]\n",
    "        \n",
    "        \"\"\"\n",
    "        Begin: train, val, test: splits & sequences\n",
    "        \"\"\"\n",
    "\n",
    "        # Split the data into train, validation, and test sets\n",
    "        train_set = df[df['participant_id'].isin(train_fold)]\n",
    "        X_train = features.loc[train_set.index, : ]\n",
    "        \n",
    "        val_set = df[df['participant_id'].isin(val_fold)]\n",
    "        X_val = features.loc[val_set.index, : ]\n",
    "        \n",
    "        test_set = df[df['participant_id'].isin(test_fold)]\n",
    "        X_test  = features.loc[test_set.index, : ]\n",
    "\n",
    "        # Convert labels to categorical format\n",
    "        num_classes = np.max(target_class) + 1  # Assuming labels start from 0\n",
    "        labels_ohe = np.eye(num_classes)[target_class]\n",
    "        \n",
    "        # Retrieve y_train, y_val, and y_test: values corresponding to same indexes, from labels_ohe\n",
    "        y_train = labels_ohe[X_train.index]\n",
    "        y_val = labels_ohe[X_val.index]\n",
    "        y_test = labels_ohe[X_test.index]\n",
    "\n",
    "#         # Print size of all sets\n",
    "#         print('Size of all sets before resetting the X indexes')\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "#         print(X_val.shape, y_val.shape)\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "\n",
    "        #reset indexes\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_val = X_val.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        \n",
    "#         # Print size of all sets after resetting the X indexes\n",
    "#         print('Size of all sets after resetting the X indexes')\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "#         print(X_val.shape, y_val.shape)\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "        \n",
    "        #### Split data into train and test sets: if k-fold cross-validation is not needed\n",
    "        ### X_train, X_test, y_train, y_test = train_test_split(features, labels_ohe, test_size=0.2, random_state=seed_value)\n",
    "        \n",
    "        \"\"\"\n",
    "        Begin: Sequence Creation as per defined 'sequence_length'(a.k.a: lookbacks)\n",
    "        \"\"\"\n",
    "        \n",
    "        X_train_sequences = [X_train[i : i + sequence_length] for i in range(len(X_train) - sequence_length + 1)]\n",
    "        y_train_sequences = y_train[sequence_length - 1 : ]\n",
    "\n",
    "        X_val_sequences = [X_val[i : i + sequence_length] for i in range(len(X_val) - sequence_length + 1)]\n",
    "        y_val_sequences = y_val[sequence_length - 1 : ]\n",
    "\n",
    "        X_test_sequences = [X_test[i : i + sequence_length] for i in range(len(X_test) - sequence_length + 1)]\n",
    "        y_test_sequences = y_test[sequence_length - 1 : ]\n",
    "        \n",
    "        \"\"\"\n",
    "        End: Sequence Creation\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        End: train, val, test: splits & sequences\n",
    "        \"\"\"\n",
    "        return num_classes, X_train, X_val, X_test, y_train, y_val, y_test, X_train_sequences, y_train_sequences, X_val_sequences, y_val_sequences, X_test_sequences, y_test_sequences\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "            results_file.write(\n",
    "                f'Exception {e} thrown during splitting the dataset for :- '\n",
    "                f'{traceback.print_exc()}'\n",
    "            )\n",
    "        pass\n",
    "\n",
    "def executeModel_LSTM(df, results_directory, seed_value = 42, sequence_lengths = 1, units = 64, dropouts = 0.2, activations = 'softmax', losses = 'categorical_crossentropy', optimizers = 'adam', epochs = 100, batch_sizes = 32):\n",
    "\n",
    "    \"\"\"\n",
    "    executeModel_LSTM(): takes in the dataFrame along with the directory specification & hyper-parameters and trains a LSTM model\n",
    "\n",
    "    Parameters:\n",
    "    - df\n",
    "    - results_directory\n",
    "    - seed_value\n",
    "    - sequence_length (a.k.a: lookback)\n",
    "    - units\n",
    "    - dropouts\n",
    "    - activations\n",
    "    - losses\n",
    "    - optimizers\n",
    "    - epochs\n",
    "    - batch_sizes\n",
    "    \"\"\"\n",
    "    # Keep count of the number of different search combinations\n",
    "    search_count = 0\n",
    "\n",
    "    for sequence_length in sequence_lengths:\n",
    "        for unit in units:\n",
    "            for dropout in dropouts:\n",
    "                for activation in activations:\n",
    "                    for loss in losses:\n",
    "                        for optimizer in optimizers:\n",
    "                            for epoch in epochs:\n",
    "                                for batch_size in batch_sizes:\n",
    "                                    try:\n",
    "                                        search_count += 1\n",
    "                                        \n",
    "                                        # # Retrieve the splits\n",
    "                                        num_classes, X_train, X_val, X_test, y_train, y_val, y_test, X_train_sequences, y_train_sequences, X_val_sequences, y_val_sequences, X_test_sequences, y_test_sequences = createDataSplits(df, results_directory, seed_value, sequence_length)\n",
    "                                        \n",
    "                                        \"\"\"\n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                        Begin: Model Architecture\n",
    "                                        \"\"\"\n",
    "                                        # # Create the LSTM model\n",
    "                                        model = Sequential()\n",
    "                                        model.add(LSTM(units = unit, input_shape = (sequence_length, X_train.shape[1])))\n",
    "                                        model.add(Dropout(dropout))\n",
    "                                        model.add(Dense(units=num_classes, activation = activation))\n",
    "\n",
    "                                        # Compile the model\n",
    "                                        model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "                                        # Train the model and capture the history data\n",
    "                                        model_history = model.fit(\n",
    "                                            np.array(X_train_sequences),\n",
    "                                            y_train_sequences,\n",
    "                                            batch_size = batch_size,\n",
    "                                            epochs = epoch,\n",
    "                                            verbose = 'auto',\n",
    "                                            validation_data=(np.array(X_val_sequences), y_val_sequences),\n",
    "                                        )\n",
    "                                        \n",
    "                                        # Obtain the training loss & accuracy data\n",
    "                                        train_loss, train_accuracy = model_history.history['loss'], model_history.history['accuracy']\n",
    "                                        \n",
    "                                        # Obtain the validation loss & accuracy data\n",
    "                                        val_loss, val_accuracy = model_history.history['val_loss'], model_history.history['val_accuracy']\n",
    "                                        \n",
    "                                        # Evaluate the model on test data\n",
    "                                        test_loss, test_accuracy = model.evaluate(np.array(X_test_sequences), y_test_sequences)\n",
    "                                        \n",
    "                                        \"\"\"\n",
    "                                        End: Model Architecture\n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                        \"\"\"\n",
    "                                        \n",
    "                                        \"\"\"\n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                            Predictions using the Model\n",
    "                                            ===========================\n",
    "\n",
    "                                            When making predictions using the trained model, the output is in the form of predicted probabilities,\n",
    "                                            indicating the likelihood of each sample belonging to each target class.\n",
    "\n",
    "                                            Predicted Probabilities (y_predict_probs):\n",
    "                                            - Shape: (#samples, #target_classes)\n",
    "                                            - Each value in y_predict_probs represents the probability of the corresponding sample being classified\n",
    "                                            into the respective class.\n",
    "\n",
    "                                            Converting Probabilities to Class Labels (y_predict):\n",
    "                                            - The y_predict array is derived by finding the index of the maximum value along a specified axis.\n",
    "                                            - It represents the predicted class label for each sample based on the highest predicted probability.\n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                        \"\"\"\n",
    "\n",
    "                                        y_predict_probs = model.predict(np.array(X_test_sequences))\n",
    "                                        y_predict = np.argmax(y_predict_probs, axis=1)  # Convert to class labels\n",
    "\n",
    "                                        report = classification_report(np.argmax(y_test_sequences, axis=1), y_predict)\n",
    "\n",
    "                                        \"\"\"\n",
    "                                            Generate the Confusion Matrix\n",
    "                                        \"\"\"\n",
    "\n",
    "                                        # Calculate the confusion matrix\n",
    "                                        conf_matrix = confusion_matrix(np.argmax(y_test_sequences, axis=1), y_predict)\n",
    "\n",
    "                                        # Get the class labels (assuming y_true and y_pred are integer class labels)\n",
    "                                        class_labels = ['Control', 'Failure_Human', 'Failure_Robot']\n",
    "\n",
    "                                        # Plot the confusion matrix using seaborn and matplotlib\n",
    "                                        plt.figure(figsize=(8, 8))\n",
    "                                        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "                                        plt.xlabel(\"Predicted\")\n",
    "                                        plt.ylabel(\"True\")\n",
    "                                        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "                                        # Save the confusion matrix as an image\n",
    "                                        plt.savefig(f'{results_directory}/confusion_matrix_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}_{batch_size}.png')\n",
    "                                        # plt.show()\n",
    "\n",
    "                                        \"\"\"\n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                        Begin: Logging \n",
    "                                        - Write all the information of the particular combination of the model to a file below\n",
    "                                        \"\"\"\n",
    "\n",
    "                                        with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "                                            results_file.write(\"\\n\")\n",
    "                                            results_file.write(f\"------------ BEGIN SEARCH : {search_count} ------------\" + \"\\n\")\n",
    "                                            results_file.write(\"------------ TYPE ------------\" + \"\\n\")\n",
    "\n",
    "                                            results_file.write(\n",
    "                                                f'Sequence Length = {sequence_length}\\n'\n",
    "                                                f'Units = {unit}\\n'\n",
    "                                                f'Dropout = {dropout}\\n'\n",
    "                                                f'Activation = {activation}\\n'\n",
    "                                                f'Loss Function = {loss}\\n'\n",
    "                                                f'Optimizer = {optimizer}\\n'\n",
    "                                                f'Epochs = {epoch}\\n'\n",
    "                                                f'Batch Size = {batch_size}\\n'\n",
    "                                                f'Seed Value = {seed_value}\\n'\n",
    "                                            )\n",
    "\n",
    "                                            results_file.write(\"------------ METRICS ------------\" + \"\\n\")\n",
    "\n",
    "                                            results_file.write(f'Training Loss: {train_loss[-1]: .4f}' + '\\n')\n",
    "                                            results_file.write(f'Training Accuracy: {train_accuracy[-1]: .4f}' + '\\n')\n",
    "                                            results_file.write(f'Validation Loss: {val_loss[-1]: .4f}' + '\\n')\n",
    "                                            results_file.write(f'Validation Accuracy: {val_accuracy[-1]: .4f}' + '\\n')\n",
    "                                            results_file.write(f'Test Loss: {test_loss:.4f}' + '\\n')\n",
    "                                            results_file.write(f'Test Accuracy: {test_accuracy:.4f}' + '\\n')\n",
    "\n",
    "                                            results_file.write(\"------------ PARAMETERS ------------\" + \"\\n\")\n",
    "\n",
    "                                            results_file.write(f'Model Parameters: {model_history.params}' + '\\n')\n",
    "                                            results_file.write(f'Model Keys: {model_history.history.keys()}' + '\\n')\n",
    "\n",
    "                                            results_file.write(\"------------ CLASSIFICATION REPORT ------------\" + \"\\n\")\n",
    "\n",
    "                                            results_file.write(report + '\\n')\n",
    "\n",
    "                                            results_file.write(\"------------ CONFUSION MATRIX ------------\" + \"\\n\")\n",
    "\n",
    "                                            results_file.write(f'Confusion matrix saved for confusion_matrix_{sequence_length}_{unit}_{dropout}_{activation}_{loss}_{optimizer}_{epoch}_{batch_size}.png' + '\\n')\n",
    "\n",
    "                                            results_file.write(f\"------------ END SEARCH : {search_count} ------------\" + \"\\n\")\n",
    "                                            results_file.write(\"\\n\")\n",
    "                                            \n",
    "                                        \"\"\"\n",
    "                                        End: Logging \n",
    "                                        ------------------------------------------------------------------------------------\n",
    "                                        \"\"\"\n",
    "                                    except Exception as e:\n",
    "                                        with open(f'{results_directory}/results_LSTM.txt', 'a') as results_file:\n",
    "                                            results_file.write(\n",
    "                                                f'Exception {e} thrown for :- \\n'\n",
    "                                                f'{traceback.print_exc()} \\n'\n",
    "                                                f'Sequence Length = {sequence_length}\\n'\n",
    "                                                f'Units = {unit}\\n'\n",
    "                                                f'Dropout = {dropout}\\n'\n",
    "                                                f'Activation = {activation}\\n'\n",
    "                                                f'Loss Function = {loss}\\n'\n",
    "                                                f'Optimizer = {optimizer}\\n'\n",
    "                                                f'Epochs = {epoch}\\n'\n",
    "                                                f'Batch Size = {batch_size}\\n'\n",
    "                                                f'Seed Value = {seed_value}\\n'\n",
    "                                            )\n",
    "#                                     break\n",
    "                                break\n",
    "                            break\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "            break\n",
    "        break\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Begin: Directories specification\n",
    "    \"\"\"\n",
    "    \n",
    "    # allParticipants dataset path\n",
    "    superBAD_df = pd.read_excel('../data/allParticipants_5fps_downsampled_preprocessed_norm.xlsx')\n",
    "    \n",
    "    # results directory - make a new folder with the day and time of the run\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    results_directory = '../results/' + 'LSTM_' + now.strftime(\"%Y-%m-%d_%H-%M-%S\") + '/'\n",
    "    \n",
    "    # Create 'results_directory' if it doesn't exist\n",
    "    if not os.path.exists(results_directory):\n",
    "        os.makedirs(results_directory)\n",
    "    \n",
    "    \"\"\"\n",
    "    End: Directories specification\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Begin: Hyperparameters definition\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [5, 10, 15]\n",
    "    units = [32, 64, 128]\n",
    "    dropouts = [0, 0.2, 0.4, 0.6]\n",
    "    activations = ['sigmoid', 'relu', 'tanh', 'softmax']\n",
    "    losses = ['mean_squared_error', 'categorical_crossentrophy', 'binary_crossentropy', 'hinge']\n",
    "    optimizers = ['SGD', 'Adam']\n",
    "    epochs = [2, 100, 250, 500]\n",
    "    batch_sizes = [8, 16, 32, 64]\n",
    "    \n",
    "    \"\"\"\n",
    "    End: Hyperparameters definition\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Begin: Call methods\n",
    "    \"\"\"\n",
    "\n",
    "    # Call your model execution function with keyword arguments\n",
    "    executeModel_LSTM(\n",
    "        superBAD_df,\n",
    "        results_directory,\n",
    "        seed_value = 42,\n",
    "        sequence_lengths = sequence_lengths,\n",
    "        units = units,\n",
    "        dropouts = dropouts,\n",
    "        activations = activations,\n",
    "        losses = losses,\n",
    "        optimizers = optimizers,\n",
    "        epochs = epochs,\n",
    "        batch_sizes = batch_sizes\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    End: Call methods\n",
    "    \"\"\"\n",
    "\"\"\"\n",
    "End: Method Definitions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37557304",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f4238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
